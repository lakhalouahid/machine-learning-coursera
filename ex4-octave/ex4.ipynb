{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3951e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%% Machine Learning Online Class - Exercise 4 Neural Network Learning\n",
    "\n",
    "%  Instructions\n",
    "%  ------------\n",
    "% \n",
    "%  This file contains code that helps you get started on the\n",
    "%  linear exercise. You will need to complete the following functions \n",
    "%  in this exericse:\n",
    "%\n",
    "%     sigmoidGradient.m\n",
    "%     randInitializeWeights.m\n",
    "%     nnCostFunction.m\n",
    "%\n",
    "%  For this exercise, you will not need to change any code in this file,\n",
    "%  or any other files other than those mentioned above.\n",
    "%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0af17ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%% Initialization\n",
    "clear ; close all; clc\n",
    "\n",
    "%% Setup the parameters you will use for this exercise\n",
    "input_layer_size  = 400;  % 20x20 Input Images of Digits\n",
    "hidden_layer_size = 25;   % 25 hidden units\n",
    "num_labels = 10;          % 10 labels, from 1 to 10   \n",
    "                          % (note that we have mapped \"0\" to label 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc67015",
   "metadata": {},
   "outputs": [],
   "source": [
    "%% =========== Part 1: Loading and Visualizing Data =============\n",
    "%  We start the exercise by first loading and visualizing the dataset. \n",
    "%  You will be working with a dataset that contains handwritten digits.\n",
    "%\n",
    "\n",
    "% Load Training Data\n",
    "fprintf('Loading and Visualizing Data ...\\n')\n",
    "\n",
    "load('ex4data1.mat');\n",
    "m = size(X, 1);\n",
    "\n",
    "% Randomly select 100 data points to display\n",
    "sel = randperm(size(X, 1));\n",
    "sel = sel(1:100);\n",
    "\n",
    "displayData(X(sel, :));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0871af39",
   "metadata": {},
   "outputs": [],
   "source": [
    "%% ================ Part 2: Loading Parameters ================\n",
    "% In this part of the exercise, we load some pre-initialized \n",
    "% neural network parameters.\n",
    "\n",
    "fprintf('\\nLoading Saved Neural Network Parameters ...\\n')\n",
    "\n",
    "% Load the weights into variables Theta1 and Theta2\n",
    "load('ex4weights.mat');\n",
    "\n",
    "% Unroll parameters \n",
    "nn_params = [Theta1(:) ; Theta2(:)];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f434227",
   "metadata": {},
   "outputs": [],
   "source": [
    "%% ================ Part 3: Compute Cost (Feedforward) ================\n",
    "%  To the neural network, you should first start by implementing the\n",
    "%  feedforward part of the neural network that returns the cost only. You\n",
    "%  should complete the code in nnCostFunction.m to return cost. After\n",
    "%  implementing the feedforward to compute the cost, you can verify that\n",
    "%  your implementation is correct by verifying that you get the same cost\n",
    "%  as us for the fixed debugging parameters.\n",
    "%\n",
    "%  We suggest implementing the feedforward cost *without* regularization\n",
    "%  first so that it will be easier for you to debug. Later, in part 4, you\n",
    "%  will get to implement the regularized cost.\n",
    "%\n",
    "fprintf('\\nFeedforward Using Neural Network ...\\n')\n",
    "\n",
    "% Weight regularization parameter (we set this to 0 here).\n",
    "lambda = 0;\n",
    "\n",
    "J = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, ...\n",
    "                   num_labels, X, y, lambda);\n",
    "\n",
    "fprintf(['Cost at parameters (loaded from ex4weights): %f '...\n",
    "         '\\n(this value should be about 0.287629)\\n'], J);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4787a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%% =============== Part 4: Implement Regularization ===============\n",
    "%  Once your cost function implementation is correct, you should now\n",
    "%  continue to implement the regularization with the cost.\n",
    "%\n",
    "\n",
    "fprintf('\\nChecking Cost Function (w/ Regularization) ... \\n')\n",
    "\n",
    "% Weight regularization parameter (we set this to 1 here).\n",
    "lambda = 1;\n",
    "\n",
    "J = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, ...\n",
    "                   num_labels, X, y, lambda);\n",
    "\n",
    "fprintf(['Cost at parameters (loaded from ex4weights): %f '...\n",
    "         '\\n(this value should be about 0.383770)\\n'], J);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0b3224",
   "metadata": {},
   "outputs": [],
   "source": [
    "%% ================ Part 5: Sigmoid Gradient  ================\n",
    "%  Before you start implementing the neural network, you will first\n",
    "%  implement the gradient for the sigmoid function. You should complete the\n",
    "%  code in the sigmoidGradient.m file.\n",
    "%\n",
    "\n",
    "fprintf('\\nEvaluating sigmoid gradient...\\n')\n",
    "\n",
    "g = sigmoidGradient([-1 -0.5 0 0.5 1]);\n",
    "fprintf('Sigmoid gradient evaluated at [-1 -0.5 0 0.5 1]:\\n  ');\n",
    "fprintf('%f ', g);\n",
    "fprintf('\\n\\n');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06809b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%% ================ Part 6: Initializing Pameters ================\n",
    "%  In this part of the exercise, you will be starting to implment a two\n",
    "%  layer neural network that classifies digits. You will start by\n",
    "%  implementing a function to initialize the weights of the neural network\n",
    "%  (randInitializeWeights.m)\n",
    "\n",
    "fprintf('\\nInitializing Neural Network Parameters ...\\n')\n",
    "\n",
    "initial_Theta1 = randInitializeWeights(input_layer_size, hidden_layer_size);\n",
    "initial_Theta2 = randInitializeWeights(hidden_layer_size, num_labels);\n",
    "\n",
    "% Unroll parameters\n",
    "initial_nn_params = [initial_Theta1(:) ; initial_Theta2(:)];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3deea9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%% =============== Part 7: Implement Backpropagation ===============\n",
    "%  Once your cost matches up with ours, you should proceed to implement the\n",
    "%  backpropagation algorithm for the neural network. You should add to the\n",
    "%  code you've written in nnCostFunction.m to return the partial\n",
    "%  derivatives of the parameters.\n",
    "%\n",
    "fprintf('\\nChecking Backpropagation... \\n');\n",
    "\n",
    "%  Check gradients by running checkNNGradients\n",
    "checkNNGradients;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95298212",
   "metadata": {},
   "outputs": [],
   "source": [
    "%% =============== Part 8: Implement Regularization ===============\n",
    "%  Once your backpropagation implementation is correct, you should now\n",
    "%  continue to implement the regularization with the cost and gradient.\n",
    "%\n",
    "\n",
    "fprintf('\\nChecking Backpropagation (w/ Regularization) ... \\n')\n",
    "\n",
    "%  Check gradients by running checkNNGradients\n",
    "lambda = 3;\n",
    "checkNNGradients(lambda);\n",
    "\n",
    "% Also output the costFunction debugging values\n",
    "debug_J  = nnCostFunction(nn_params, input_layer_size, ...\n",
    "                          hidden_layer_size, num_labels, X, y, lambda);\n",
    "\n",
    "fprintf(['\\n\\nCost at (fixed) debugging parameters (w/ lambda = %f): %f ' ...\n",
    "         '\\n(for lambda = 3, this value should be about 0.576051)\\n\\n'], lambda, debug_J);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f304c848",
   "metadata": {},
   "outputs": [],
   "source": [
    "%% =================== Part 8: Training NN ===================\n",
    "%  You have now implemented all the code necessary to train a neural \n",
    "%  network. To train your neural network, we will now use \"fmincg\", which\n",
    "%  is a function which works similarly to \"fminunc\". Recall that these\n",
    "%  advanced optimizers are able to train our cost functions efficiently as\n",
    "%  long as we provide them with the gradient computations.\n",
    "%\n",
    "fprintf('\\nTraining Neural Network... \\n')\n",
    "\n",
    "%  After you have completed the assignment, change the MaxIter to a larger\n",
    "%  value to see how more training helps.\n",
    "options = optimset('MaxIter', 50);\n",
    "\n",
    "%  You should also try different values of lambda\n",
    "lambda = 1;\n",
    "\n",
    "% Create \"short hand\" for the cost function to be minimized\n",
    "costFunction = @(p) nnCostFunction(p, ...\n",
    "                                   input_layer_size, ...\n",
    "                                   hidden_layer_size, ...\n",
    "                                   num_labels, X, y, lambda);\n",
    "\n",
    "% Now, costFunction is a function that takes in only one argument (the\n",
    "% neural network parameters)\n",
    "[nn_params, cost] = fmincg(costFunction, initial_nn_params, options);\n",
    "\n",
    "% Obtain Theta1 and Theta2 back from nn_params\n",
    "Theta1 = reshape(nn_params(1:hidden_layer_size * (input_layer_size + 1)), ...\n",
    "                 hidden_layer_size, (input_layer_size + 1));\n",
    "\n",
    "Theta2 = reshape(nn_params((1 + (hidden_layer_size * (input_layer_size + 1))):end), ...\n",
    "                 num_labels, (hidden_layer_size + 1));\n",
    "\n",
    "\n",
    "%% ================= Part 9: Visualize Weights =================\n",
    "%  You can now \"visualize\" what the neural network is learning by \n",
    "%  displaying the hidden units to see what features they are capturing in \n",
    "%  the data.\n",
    "\n",
    "fprintf('\\nVisualizing Neural Network... \\n')\n",
    "\n",
    "displayData(Theta1(:, 2:end));\n",
    "\n",
    "%% ================= Part 10: Implement Predict =================\n",
    "%  After training the neural network, we would like to use it to predict\n",
    "%  the labels. You will now implement the \"predict\" function to use the\n",
    "%  neural network to predict the labels of the training set. This lets\n",
    "%  you compute the training set accuracy.\n",
    "\n",
    "pred = predict(Theta1, Theta2, X);\n",
    "\n",
    "fprintf('\\nTraining Set Accuracy: %f\\n', mean(double(pred == y)) * 100);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Octave",
   "language": "octave",
   "name": "octave"
  },
  "language_info": {
   "file_extension": ".m",
   "help_links": [
    {
     "text": "GNU Octave",
     "url": "https://www.gnu.org/software/octave/support.html"
    },
    {
     "text": "Octave Kernel",
     "url": "https://github.com/Calysto/octave_kernel"
    },
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-octave",
   "name": "octave",
   "version": "6.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
