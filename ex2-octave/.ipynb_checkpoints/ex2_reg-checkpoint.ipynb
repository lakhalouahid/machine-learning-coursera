{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03de11ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%% Machine Learning Online Class - Exercise 2: Logistic Regression\n",
    "%\n",
    "%  Instructions\n",
    "%  ------------\n",
    "%\n",
    "%  This file contains code that helps you get started on the second part\n",
    "%  of the exercise which covers regularization with logistic regression.\n",
    "%\n",
    "%  You will need to complete the following functions in this exericse:\n",
    "%\n",
    "%     sigmoid.m\n",
    "%     costFunction.m\n",
    "%     predict.m\n",
    "%     costFunctionReg.m\n",
    "%\n",
    "%  For this exercise, you will not need to change any code in this file,\n",
    "%  or any other files other than those mentioned above.\n",
    "%\n",
    "\n",
    "%% Initialization\n",
    "graphics_toolkit(\"gnuplot\");\n",
    "plot(1:1000)\n",
    "graphics_toolkit(\"gnuplot\");\n",
    "clear ; close all; clc\n",
    "\n",
    "%% Load Data\n",
    "%  The first two columns contains the X values and the third column\n",
    "%  contains the label (y).\n",
    "\n",
    "data = load('ex2data2.txt');\n",
    "X = data(:, [1, 2]); y = data(:, 3);\n",
    "\n",
    "plotData(X, y);\n",
    "\n",
    "% Put some labels\n",
    "hold on;\n",
    "\n",
    "% Labels and Legend\n",
    "xlabel('Microchip Test 1')\n",
    "ylabel('Microchip Test 2')\n",
    "\n",
    "% Specified in plot order\n",
    "legend('y = 1', 'y = 0')\n",
    "hold off;\n",
    "\n",
    "\n",
    "%% =========== Part 1: Regularized Logistic Regression ============\n",
    "%  In this part, you are given a dataset with data points that are not\n",
    "%  linearly separable. However, you would still like to use logistic\n",
    "%  regression to classify the data points.\n",
    "%\n",
    "%  To do so, you introduce more features to use -- in particular, you add\n",
    "%  polynomial features to our data matrix (similar to polynomial\n",
    "%  regression).\n",
    "%\n",
    "\n",
    "% Add Polynomial Features\n",
    "\n",
    "% Note that mapFeature also adds a column of ones for us, so the intercept\n",
    "% term is handled\n",
    "X = mapFeature(X(:,1), X(:,2));\n",
    "\n",
    "% Initialize fitting parameters\n",
    "initial_theta = zeros(size(X, 2), 1);\n",
    "\n",
    "% Set regularization parameter lambda to 1\n",
    "lambda = 1;\n",
    "\n",
    "% Compute and display initial cost and gradient for regularized logistic\n",
    "% regression\n",
    "[cost, grad] = costFunctionReg(initial_theta, X, y, lambda);\n",
    "\n",
    "fprintf('Cost at initial theta (zeros): %f\\n', cost);\n",
    "fprintf('Expected cost (approx): 0.693\\n');\n",
    "fprintf('Gradient at initial theta (zeros) - first five values only:\\n');\n",
    "fprintf(' %f \\n', grad(1:5));\n",
    "fprintf('Expected gradients (approx) - first five values only:\\n');\n",
    "fprintf(' 0.0085\\n 0.0188\\n 0.0001\\n 0.0503\\n 0.0115\\n');\n",
    "\n",
    "fprintf('\\nProgram paused. Press enter to continue.\\n');\n",
    "pause;\n",
    "\n",
    "% Compute and display cost and gradient\n",
    "% with all-ones theta and lambda = 10\n",
    "test_theta = ones(size(X,2),1);\n",
    "[cost, grad] = costFunctionReg(test_theta, X, y, 10);\n",
    "\n",
    "fprintf('\\nCost at test theta (with lambda = 10): %f\\n', cost);\n",
    "fprintf('Expected cost (approx): 3.16\\n');\n",
    "fprintf('Gradient at test theta - first five values only:\\n');\n",
    "fprintf(' %f \\n', grad(1:5));\n",
    "fprintf('Expected gradients (approx) - first five values only:\\n');\n",
    "fprintf(' 0.3460\\n 0.1614\\n 0.1948\\n 0.2269\\n 0.0922\\n');\n",
    "\n",
    "fprintf('\\nProgram paused. Press enter to continue.\\n');\n",
    "pause;\n",
    "\n",
    "%% ============= Part 2: Regularization and Accuracies =============\n",
    "%  Optional Exercise:\n",
    "%  In this part, you will get to try different values of lambda and\n",
    "%  see how regularization affects the decision coundart\n",
    "%\n",
    "%  Try the following values of lambda (0, 1, 10, 100).\n",
    "%\n",
    "%  How does the decision boundary change when you vary lambda? How does\n",
    "%  the training set accuracy vary?\n",
    "%\n",
    "\n",
    "% Initialize fitting parameters\n",
    "initial_theta = zeros(size(X, 2), 1);\n",
    "\n",
    "% Set regularization parameter lambda to 1 (you should vary this)\n",
    "lambda = 0.01;\n",
    "\n",
    "% Set Options\n",
    "options = optimset('GradObj', 'on', 'MaxIter', 400);\n",
    "\n",
    "% Optimize\n",
    "[theta, J, exit_flag] = ...\n",
    "\tfminunc(@(t)(costFunctionReg(t, X, y, lambda)), initial_theta, options);\n",
    "\n",
    "% Plot Boundary\n",
    "plotDecisionBoundary(theta, X, y);\n",
    "hold on;\n",
    "title(sprintf('lambda = %g', lambda))\n",
    "\n",
    "% Labels and Legend\n",
    "xlabel('Microchip Test 1')\n",
    "ylabel('Microchip Test 2')\n",
    "\n",
    "legend('y = 1', 'y = 0', 'Decision boundary')\n",
    "hold off;\n",
    "\n",
    "% Compute accuracy on our training set\n",
    "p = predict(theta, X);\n",
    "\n",
    "fprintf('Train Accuracy: %f\\n', mean(double(p == y)) * 100);\n",
    "fprintf('Expected accuracy (with lambda = 1): 83.1 (approx)\\n');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Octave",
   "language": "octave",
   "name": "octave"
  },
  "language_info": {
   "file_extension": ".m",
   "help_links": [
    {
     "text": "GNU Octave",
     "url": "https://www.gnu.org/software/octave/support.html"
    },
    {
     "text": "Octave Kernel",
     "url": "https://github.com/Calysto/octave_kernel"
    },
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-octave",
   "name": "octave",
   "version": "6.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
